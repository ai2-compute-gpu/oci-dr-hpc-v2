{
  "recommendations": {
    "gpu_count_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0001-0001",
        "issue": "GPU count mismatch detected. Expected count not met (found: {gpu_count})",
        "suggestion": "Verify GPU hardware installation and driver status",
        "commands": [
          "nvidia-smi",
          "lspci | grep -i nvidia",
          "dmesg | grep -i nvidia",
          "sudo nvidia-smi -pm 1"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GPU count check passed ({gpu_count} GPUs detected)",
        "suggestion": "GPU hardware is properly detected and configured",
        "commands": [
          "nvidia-smi -q",
          "nvidia-smi topo -m"
        ]
      }
    },
    "gpu_mode_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0001-0002",
        "issue": "GPU MIG mode configuration violation detected on GPUs: {enabled_gpu_indexes}",
        "suggestion": "Disable MIG mode on affected GPUs or verify that MIG configuration meets workload requirements",
        "commands": [
          "nvidia-smi --query-gpu=index,mig.mode.current --format=csv,noheader",
          "sudo nvidia-smi -mig 0",
          "sudo nvidia-smi -i {enabled_gpu_indexes} -mig 0",
          "nvidia-smi mig -lgip",
          "nvidia-smi -q -i {enabled_gpu_indexes}"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/mig-user-guide/",
          "https://developer.nvidia.com/blog/getting-the-most-out-of-the-a100-gpu-with-multi-instance-gpu/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GPU MIG mode check passed - all GPUs have acceptable mode configuration",
        "suggestion": "GPU MIG mode configuration is compliant",
        "commands": [
          "nvidia-smi --query-gpu=index,mig.mode.current --format=csv,noheader"
        ]
      }
    },
    "gpu_clk_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0011-0001",
        "issue": "GPU clock speeds below acceptable threshold (found: {clock_speed})",
        "suggestion": "Verify GPU performance state and check for thermal throttling",
        "commands": [
          "nvidia-smi --query-gpu=clocks.current.graphics --format=csv,noheader,nounits",
          "nvidia-smi -q -d CLOCK",
          "nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv,noheader",
          "nvidia-smi --query-gpu=pstate --format=csv,noheader"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://developer.nvidia.com/nvidia-system-management-interface"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GPU clock speed check passed ({clock_speed})",
        "suggestion": "GPU clock speeds are within acceptable range",
        "commands": [
          "nvidia-smi --query-gpu=clocks.current.graphics --format=csv,noheader,nounits",
          "nvidia-smi -q -d CLOCK"
        ]
      }
    },
    "pcie_error_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0002-0001",
        "issue": "PCIe errors detected in system logs",
        "suggestion": "Check PCIe bus health and reseat hardware if necessary",
        "commands": [
          "dmesg | grep -i pcie",
          "dmesg | grep -i 'corrected error'",
          "dmesg | grep -i 'uncorrectable error'",
          "lspci -tv",
          "sudo pcieport-error-inject"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://www.kernel.org/doc/Documentation/PCI/pci-error-recovery.txt"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "PCIe error check passed",
        "suggestion": "PCIe bus appears healthy with no errors detected",
        "commands": [
          "lspci -tv",
          "dmesg | tail -50"
        ]
      }
    },
    "rdma_nics_count": {
      "fail": {
        "type": "warning",
        "fault_code": "HPCGPU-0003-0001",
        "issue": "RDMA NIC count mismatch (found: {num_rdma_nics})",
        "suggestion": "Verify RDMA hardware installation and driver configuration",
        "commands": [
          "ibstat",
          "ibv_devices",
          "lspci | grep -i mellanox",
          "rdma link show",
          "systemctl status openibd"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://docs.mellanox.com/display/MLNXOFEDv461000/"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "RDMA NIC count check passed ({num_rdma_nics} NICs detected)",
        "suggestion": "RDMA hardware is properly detected and configured",
        "commands": [
          "ibstat",
          "ibv_devinfo",
          "rdma link show"
        ]
      }
    },
    "rx_discards_check": {
      "fail": {
        "type": "warning",
        "fault_code": "HPCGPU-0004-0001",
        "issue": "RX discards exceeded the specified threshold for {failed_interfaces}",
        "suggestion": "TODO: Suggestion",
        "commands": [
          "TODO: commands"
        ],
        "references": [
          "TODO: references"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "RX discards check passed",
        "suggestion": "RX discards does not exceed a specified threshold",
        "commands": [
          "sudo ethtool -S {interface} | grep rx_prio.*_discards"
        ]
      }
    },
    "gid_index_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0005-0001",
        "issue": "GID index on a system is not in range",
        "suggestion": "Reboot the host and re-run the check. If the issue persists, verify that you're using the correct oracle-cloud-agent plugin (v1.46+) and image. If the problem continues, contact your OCI support team.",
        "commands": [
          "sudo yum info oracle-cloud-agent",
          "sudo yum install -y oracle-cloud-agent"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/manage-plugins.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GID index check passed",
        "suggestion": "GID index on a system matches expected values",
        "commands": [
          "sudo show_gids | tail -n +3 | head -n -1"
        ]
      }
    },
    "link_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0008-0001",
        "issue": "RDMA link check failed - link parameters do not meet expected values",
        "suggestion": "Check RDMA link health, verify cable connections, and inspect link parameters",
        "commands": [
          "ibstat",
          "ibv_devices",
          "rdma link show",
          "sudo mlxlink -d mlx5_0 --show_module --show_counters",
          "sudo ibdev2netdev",
          "lspci | grep -i mellanox"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://docs.mellanox.com/display/MLNXOFEDv461000/",
          "https://community.mellanox.com/s/article/understanding-mlx-link-utility"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "RDMA link check passed",
        "suggestion": "All RDMA links are healthy with expected parameters",
        "commands": [
          "ibstat",
          "rdma link show",
          "sudo mlxlink -d mlx5_0 --show_module"
        ]
      }
    },
    "eth_link_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0007-0001",
        "issue": "Ethernet link check failed - link parameters do not meet expected values",
        "suggestion": "Check Ethernet link health, verify cable connections, and inspect link parameters for 100GbE RoCE interfaces",
        "commands": [
          "sudo ibdev2netdev",
          "sudo mst status -v",
          "ip link show",
          "sudo mlxlink -d {device} --show_module --show_counters --show_eye",
          "ethtool {interface}",
          "lspci | grep -i ethernet"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://docs.mellanox.com/display/MLNXOFEDv461000/",
          "https://community.mellanox.com/s/article/understanding-mlx-link-utility",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "Ethernet link check passed",
        "suggestion": "All Ethernet links are healthy with expected parameters",
        "commands": [
          "sudo ibdev2netdev",
          "ip link show",
          "ethtool {interface}"
        ]
      }
    },
    "auth_check": {
      "fail": {
        "type": "warning",
        "fault_code": "HPCGPU-0008-0001",
        "issue": "RDMA interface authentication check failed - some interfaces are not authenticated",
        "suggestion": "If the check reports an error for one of the RDMA links, rerun the test, as a reconfiguration may have been in progress during certificate rotation. If the issue persists and both test attempts fail, proceed to restart the oracle-cloud-agent plugin.",
        "commands": [
          "sudo wpa_cli -i {interface} status",
          "sudo systemctl status wpa_supplicant",
          "sudo systemctl restart oracle-cloud-agent"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://w1.fi/wpa_supplicant/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "Authentication check passed",
        "suggestion": "All RDMA interfaces are properly authenticated",
        "commands": [
          "sudo wpa_cli -i {interface} status",
          "sudo ibdev2netdev"
        ]
      }
    },
    "sram_error_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0006-0001",
        "issue": "GPU SRAM uncorrectable errors detected (max: {max_uncorrectable}). This indicates serious hardware memory corruption that can cause system instability and data loss.",
        "suggestion": "Immediately investigate GPU memory health. Consider replacing affected GPUs as uncorrectable errors indicate hardware failure. Stop critical workloads until issue is resolved.",
        "commands": [
          "sudo nvidia-smi -q | grep -A 3 Aggregate | grep Correctable",
          "sudo nvidia-smi -q | grep -A 3 Aggregate | grep Uncorrectable",
          "nvidia-smi --query-gpu=memory.total,memory.free,memory.used --format=csv",
          "nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv",
          "dmesg | grep -i 'gpu\\|nvidia\\|memory'"
        ],
        "references": [
          "https://docs.nvidia.com/deploy/gpu-debug-guidelines/index.html",
          "https://developer.nvidia.com/nvidia-system-management-interface"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GPU SRAM error levels are within acceptable limits (uncorrectable: {max_uncorrectable}, correctable: {max_correctable})",
        "suggestion": "Continue monitoring GPU memory health as part of regular maintenance",
        "commands": [
          "sudo nvidia-smi -q | grep -A 3 Aggregate | grep Correctable",
          "sudo nvidia-smi -q | grep -A 3 Aggregate | grep Uncorrectable"
        ]
      }
    },
    "gpu_driver_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0007-0001",
        "issue": "GPU driver version validation failed - {driver_version} is blacklisted or has issues",
        "suggestion": "Update to a supported GPU driver version or investigate driver installation issues",
        "commands": [
          "nvidia-smi --query-gpu=driver_version --format=csv,noheader",
          "sudo apt update && sudo apt install nvidia-driver-535",
          "sudo systemctl reboot",
          "dkms status",
          "modinfo nvidia"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://developer.nvidia.com/cuda-downloads"
        ]
      },
      "warn": {
        "type": "warning",
        "fault_code": "HPCGPU-0007-0002",
        "issue": "GPU driver version {driver_version} is unsupported but not blacklisted",
        "suggestion": "Consider updating to a known supported driver version for optimal performance and compatibility",
        "commands": [
          "nvidia-smi --query-gpu=driver_version --format=csv,noheader",
          "nvidia-smi -q",
          "sudo apt list --installed | grep nvidia"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "GPU driver version check passed - {driver_version} is supported",
        "suggestion": "GPU driver is properly installed and supported",
        "commands": [
          "nvidia-smi",
          "nvidia-smi -q",
          "nvidia-smi topo -m"
        ]
      }
    },
    "peermem_module_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0008-0001",
        "issue": "NVIDIA Peer Memory module (nvidia_peermem) is not loaded. This module is required for optimal GPU-to-GPU communication and peer memory access.",
        "suggestion": "Load the nvidia_peermem kernel module to enable GPU peer memory access, which is crucial for multi-GPU workloads and direct GPU-to-GPU data transfers",
        "commands": [
          "sudo lsmod | grep nvidia_peermem",
          "sudo modprobe nvidia_peermem",
          "sudo systemctl restart nvidia-peermem",
          "lsmod | grep nvidia",
          "dmesg | grep -i peermem",
          "nvidia-smi topo -p2p r"
        ],
        "references": [
          "https://docs.nvidia.com/cuda/gpudirect-rdma/index.html",
          "https://developer.nvidia.com/gpudirect",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "NVIDIA Peer Memory module check passed - nvidia_peermem module is loaded",
        "suggestion": "GPU peer memory access is properly configured for optimal multi-GPU performance",
        "commands": [
          "sudo lsmod | grep nvidia_peermem",
          "nvidia-smi topo -p2p r"
        ]
      }
    },
    "nvlink_speed_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0009-0001",
        "issue": "NVLink speed or count check failed - GPU interconnect links do not meet expected performance requirements",
        "suggestion": "Check NVLink health, verify GPU interconnect topology, inspect link parameters, and ensure proper GPU seating and cable connections. NVLink issues can severely impact multi-GPU workload performance.",
        "commands": [
          "nvidia-smi nvlink -s",
          "nvidia-smi topo -m",
          "nvidia-smi topo -p2p r",
          "nvidia-smi --query-gpu=index,name,pci.bus_id --format=csv",
          "nvidia-smi nvlink --status",
          "dmesg | grep -i nvlink",
          "nvidia-smi --query-gpu=pstate,temperature.gpu,power.draw --format=csv",
          "lspci | grep -i nvidia"
        ],
        "references": [
          "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#nvlink",
          "https://developer.nvidia.com/nvlink",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.nvidia.com/datacenter/tesla/nvlink-fabric-manager-user-guide/"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "NVLink speed and count check passed - all GPU interconnects meet performance requirements",
        "suggestion": "NVLink topology is properly configured for optimal GPU-to-GPU communication. Your multi-GPU workloads should achieve maximum bandwidth.",
        "commands": [
          "nvidia-smi nvlink -s",
          "nvidia-smi topo -m",
          "nvidia-smi topo -p2p r"
        ]
      }
    },
    "eth0_presence_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0010-0001",
        "issue": "eth0 network interface is missing or not detected. This critical network interface is required for system connectivity and management operations.",
        "suggestion": "Reboot node and see if issue persists. If persists, investigate network interface configuration, check if eth0 is properly configured or renamed, verify network drivers are loaded, and ensure the primary network interface is available for system operations.",
        "commands": [
          "ip addr show",
          "ip link show",
          "nmcli connection show",
          "systemctl status NetworkManager",
          "dmesg | grep -i 'eth0\\|network'",
          "lspci | grep -i ethernet",
          "cat /proc/net/dev",
          "ls -la /sys/class/net/",
          "sudo systemctl restart NetworkManager"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/overview.htm",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringmultipleVNICs.htm",
          "https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/",
          "https://netplan.io/reference/"
        ]
      },
      "pass": {
        "type": "info",
        "message": "eth0 network interface is present and properly configured."
      }
    },
    "cdfp_cable_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0010-0001",
        "issue": "CDFP cable connection mismatch detected. GPU PCI addresses and module IDs do not match expected configuration",
        "suggestion": "Verify CDFP cable connections between GPUs match the expected mapping. Check physical cable connections and GPU seating",
        "commands": [
          "nvidia-smi --query-gpu=pci.bus_id --format=csv,noheader",
          "nvidia-smi --query-gpu=module_id --format=csv,noheader",
          "nvidia-smi -q | grep -E '(Bus Id|Module ID)'",
          "nvidia-smi topo -m",
          "lspci | grep -i nvidia"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "CDFP cable check passed - all GPU connections are properly configured",
        "suggestion": "CDFP cables are correctly connected and GPU topology matches expected configuration",
        "commands": [
          "nvidia-smi --query-gpu=pci.bus_id,module_id --format=csv,noheader",
          "nvidia-smi topo -m"
        ]
      }
    },
    "fabricmanager_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0011-0001",
        "issue": "NVIDIA Fabric Manager service is not running. This service is required for proper GPU fabric management and multi-GPU communication in H100 systems.",
        "suggestion": "Start the nvidia-fabricmanager service to enable proper GPU fabric coordination. This service manages GPU interconnect topology and is essential for multi-GPU workloads.",
        "commands": [
          "sudo systemctl status nvidia-fabricmanager",
          "sudo systemctl start nvidia-fabricmanager",
          "sudo systemctl enable nvidia-fabricmanager",
          "sudo journalctl -u nvidia-fabricmanager -f",
          "nvidia-smi -q | grep -i fabric",
          "ps aux | grep fabricmanager"
        ],
        "references": [
          "https://docs.nvidia.com/datacenter/tesla/nvlink-fabric-manager-user-guide/",
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "NVIDIA Fabric Manager service is running and properly configured",
        "suggestion": "GPU fabric management is active and ready for multi-GPU workloads. The service is managing GPU interconnect topology correctly.",
        "commands": [
          "sudo systemctl status nvidia-fabricmanager",
          "nvidia-smi -q | grep -i fabric",
          "nvidia-smi topo -m"
        ]
      }
    },
    "hca_error_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0011-0001",
        "issue": "Fatal MLX5 errors were detected in the system logs. These errors may lead to job startup failures or cause jobs to crash during execution.",
        "suggestion": "Clear dmesg and reboot the node. If the problem persists,return the node to OCI",
        "commands": [
          "sudo dmesg -T | grep -i mlx5 | grep -i fatal"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://docs.mellanox.com/display/MLNXOFEDv461000/",
          "https://community.mellanox.com/s/article/understanding-mlx-link-utility",
          "https://docs.mellanox.com/display/winofedv28100/Troubleshooting"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "HCA error check passed - no MLX5 fatal errors detected",
        "suggestion": "RDMA hardware appears healthy with no critical errors in system logs",
        "commands": [
          "ibstat",
          "rdma link show",
          "dmesg -T | tail -50"
        ]
      }
    },
    "missing_interface_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0012-0001",
        "issue": "Missing PCIe interfaces detected ({missing_count} interface(s) with revision 'ff'). This typically indicates failed or missing hardware components that may cause system instability.",
        "suggestion": "Reboot the node. If one or more components show up missing within a day, return to OCI. If it fails to reboot, terminate and send it to OCI.",
        "commands": [
          "lspci | grep -i 'rev ff'",
          "dmesg | grep -i pci"
        ],
        "references": [
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm",
          "https://www.kernel.org/doc/Documentation/PCI/pci.txt",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/managinginstances.htm"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "Missing interface check passed - no missing PCIe interfaces detected",
        "suggestion": "All PCIe interfaces are properly detected and functioning correctly",
        "commands": [
          "lspci",
          "lspci -tv"
        ]
      }
    },
    "gpu_xid_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0016-0001",
        "issue": "Critical GPU XID errors detected in system logs. XID errors indicate serious GPU hardware or software issues that can cause system instability and data corruption.",
        "suggestion": "Investigate and resolve GPU XID errors immediately. These errors may indicate GPU hardware failure, driver issues, or system configuration problems that require immediate attention.",
        "commands": [
          "sudo dmesg | grep -i 'NVRM: Xid'",
          "nvidia-smi -q",
          "nvidia-smi -x -q",
          "sudo journalctl -b | grep -i nvidia",
          "nvidia-bug-report.sh",
          "lspci -vvv | grep -A 20 -B 5 -i nvidia"
        ],
        "references": [
          "https://docs.nvidia.com/deploy/xid-errors/index.html",
          "https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/",
          "https://developer.nvidia.com/nvidia-system-management-interface",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm"
        ]
      },
      "warn": {
        "type": "warning",
        "fault_code": "HPCGPU-0016-0002",
        "issue": "GPU XID warning errors detected in system logs. While not immediately critical, these warnings may indicate potential issues that should be monitored.",
        "suggestion": "Monitor GPU XID warnings and investigate if they persist or increase in frequency. Consider updating GPU drivers or checking system configuration.",
        "commands": [
          "sudo dmesg | grep -i 'NVRM: Xid'",
          "nvidia-smi -q -d MEMORY,ECC,TEMPERATURE,POWER",
          "watch -n 1 'nvidia-smi'",
          "nvidia-smi --query-gpu=gpu_name,driver_version,temperature.gpu,power.draw --format=csv"
        ]
        ]
      },
      "pass": {
        "type": "info",
        "issue": "No GPU XID errors found in system logs",
        "suggestion": "GPU hardware is operating normally without XID errors. Continue monitoring system logs for any future GPU-related issues.",
        "commands": [
          "nvidia-smi -q",
          "nvidia-smi --query-gpu=name,driver_version,temperature.gpu --format=csv"
        ]
        ]
      }
    },
    "max_acc_check": {
      "fail": {
        "type": "critical",
        "fault_code": "HPCGPU-0017-0001",
        "issue": "MAX_ACC_OUT_READ and/or ADVANCED_PCI_SETTINGS configuration is incorrect for optimal data transfer rates on devices: {failed_devices}. On H100 systems with DGX OS 6.0, incorrect CX-7 controller settings can result in reduced performance.",
        "suggestion": "Verify and correct the MAX_ACC_OUT_READ setting (must be 0, 44, or 128) and ensure ADVANCED_PCI_SETTINGS is set to True. These settings are critical for optimal RDMA performance on H100 systems.",
        "commands": [
          "sudo /usr/bin/mlxconfig -d <pci_device> query | grep -E 'MAX_ACC_OUT_READ|ADVANCED_PCI_SETTINGS'",
          "sudo /usr/bin/mlxconfig -d <pci_device> set MAX_ACC_OUT_READ=44",
          "sudo /usr/bin/mlxconfig -d <pci_device> set ADVANCED_PCI_SETTINGS=True",
          "lspci | grep -i mellanox",
          "ibstat"
        ],
        "references": [
          "https://docs.mellanox.com/display/MLNXOFEDv461000/",
          "https://docs.nvidia.com/networking/display/MLNXOFEDv461000/mlxconfig",
          "https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/configuringrdma.htm",
          "https://community.mellanox.com/s/article/understanding-mlx-link-utility"
        ]
      },
      "pass": {
        "type": "info",
        "issue": "MAX_ACC_OUT_READ and ADVANCED_PCI_SETTINGS are correctly configured for optimal data transfer rates",
        "suggestion": "RDMA NIC configuration is optimal for high-performance data transfers. All CX-7 controller settings meet recommended values.",
        "commands": [
          "sudo /usr/bin/mlxconfig -d <pci_device> query",
          "ibstat",
          "rdma link show"
        ]
      }
    }
  },
  "summary_templates": {
    "no_issues": "All diagnostic tests passed. Your HPC environment appears healthy.",
    "has_issues": "Found {total_issues} issue(s) requiring attention: {critical_count} critical, {warning_count} warning"
  }
}